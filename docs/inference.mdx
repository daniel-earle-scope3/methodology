---
title: "Inference"
description: "Methodology for calculating the amortized emissions from executing inference on AI models"
---

## Overview

To model inference requires understanding the emissions per inference for a model on a particular host given certain parameters. There are *many* factors that determine how a model is executed including [batching strategies](https://www.anyscale.com/blog/continuous-batching-llm-inference#continuous-batching), [paged attention](https://blog.vllm.ai/2023/06/20/vllm.html), and optimizations like quantization. The intention of this methodology is to provide a framework that both calculates and predicts the emissions cost of inference with the flexibility to include future optimizations as they appear.

The most accurate way to calculate inference emissions is to measure the actual energy use of the model running on real-world hardware. This is rarely possible. Using a controlled hardware environment for model testing, for instance as proposed by [Efficiency Pentathlon: A Standardized Arena for Efficiency Evaluation](https://arxiv.org/pdf/2307.09701), will not account for optimizations built for specific production hardware or real-world workloads.

Inference speed may be limited by the framework or by compute. Maximum performance will be achieved when models are not limited by [framework overhead](https://arxiv.org/pdf/2302.06117).

## Characterizing an inference job

### Model

The model is defined by various parameters:
- The base model used including the current [amortized training cost per inference](/training#amortization-of-impact-across-use-life)
- The current amortized fine-tuning cost per inference if applicable
- Quantization or other optimizations applied to the model
- The task being performed (eg image generation, text generation, text to speech, speech to text)

### Technical infrastructure

The datacenter:
- PUE
- WUE
- Physical location (which grid)
- On-site generation and storage (impacts 247 CFE analysis)

The servers:
- Usage energy (W)
- Embodied emissions
- GPUs per server

The GPUs:
- Make/model

Supporting services:
- RAG database or other context generation

### Parameters

Parameters will depend on the task and may not be fully deterministic. For instance, in image generation, the parameters might be the desired output resolution (1024x1024) and the prompt. In text generation, the input prompt doesn't determine the length or computational complexity of producing the output.

#### Image Generation
- Input prompt length in tokens
- Output width and height
- Input image(s) width/height


#### Text Generation
- Input prompt length in tokens
- Input image(s) width/height

## Calculation of inference emissions

```
Emissions per inference = (amortized training emissions) + (amortized fine-tuning emissions) +
                      (inference operational emissions) + (inference embodied emissions)
Inference embodied emissions = (inference cluster embodied emissions per hour)

